{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b20ac7b-14a0-4bb2-8305-b7d3b69fe56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # For creating plots\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6330f0-9dc9-4552-bf14-88b00af88a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGarmentInitial():\n",
    "    '''retrieve the initial data for the garment dataset from the csv file'''\n",
    "    import numpy as np\n",
    "    #get data from file\n",
    "    initDatasetDate = np.genfromtxt(\"gwp_assessment.csv\", delimiter=\",\", dtype=\"U25\", encoding=None, missing_values=None, filling_values=None, skip_header=1, usecols=(0))\n",
    "    initDatasetCategorical = np.genfromtxt(\"gwp_assessment.csv\", delimiter=\",\", dtype=\"U25\", encoding=None, missing_values=None, filling_values=None, skip_header=1, usecols=(1,2,3))\n",
    "    initDatasetNumerical = np.genfromtxt(\"gwp_assessment.csv\", delimiter=\",\", dtype=\"f8\", encoding=None, missing_values=None, filling_values=None, skip_header=1, usecols=(range(4,14)))\n",
    "    target = np.genfromtxt(\"gwp_assessment.csv\", delimiter=\",\", dtype=\"f8\", encoding=None, missing_values=None, filling_values=None, skip_header=1, usecols=(14))\n",
    "    return initDatasetDate, initDatasetCategorical, initDatasetNumerical, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb63e52-5f31-41e9-a71e-09888e667592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeAboveMultiple(dataToImpute):\n",
    "    '''perform a simple imputation on multiple columns of data, replacing missing values with the value directly above them'''\n",
    "    #for each column\n",
    "    for i in range(len(dataToImpute[1])):\n",
    "        #for each row\n",
    "        for x in range(len(dataToImpute)):\n",
    "            if (dataToImpute[x,i] == ''):\n",
    "                dataToImpute[x,i] = dataToImpute[x-1,i]\n",
    "    return dataToImpute\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9587ea-ccc3-4f3d-b1bd-4816c1673512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateImputeAbove(dataToImpute):\n",
    "    '''perform directly above imputation for the date column'''\n",
    "    #for each row\n",
    "    for x in range(len(dataToImpute)):\n",
    "        if (dataToImpute[x] == ''):\n",
    "            dataToImpute[x] = dataToImpute[x-1]\n",
    "    return dataToImpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36ea34d-f96f-4d2d-add1-b49aa83b36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def garmentDetectAndEncodeDate(dataToEncode):\n",
    "    '''convert the date string into 3 seperate columns, and then use encoding to make them readable my machine learning algorithms'''\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    import numpy as np\n",
    "    #generate day, month and year arrays from date\n",
    "    #month/day/year\n",
    "    day = []\n",
    "    month = []\n",
    "    year = []\n",
    "    for x in range(len(dataToEncode)):\n",
    "        splitDate = (dataToEncode[x]).split('/')\n",
    "        day.append([splitDate[1]])\n",
    "        month.append([splitDate[0]])\n",
    "        year.append([splitDate[2]])\n",
    "    day = np.array(day)\n",
    "    month = np.array(month)\n",
    "    year = np.array(year)\n",
    "    datasetDate = np.concatenate([day,month,year], axis=1)\n",
    "    #print(datasetDate.shape)\n",
    "    \n",
    "    #ordinal encoding\n",
    "    for i in [0]:\n",
    "        oec = OrdinalEncoder(categories='auto', dtype=float)\n",
    "        oec.fit(datasetDate[:, [i]])\n",
    "\n",
    "        # replace original data in the dataset with encoded values\n",
    "        datasetDate[:, i] = oec.transform(datasetDate[:, [i]]).flatten()\n",
    "        \n",
    "    #one hot encoding\n",
    "    for i in [2,1]:\n",
    "        enc = OneHotEncoder(categories='auto', dtype=float, sparse=False)\n",
    "        enc.fit(datasetDate[:, [i]])\n",
    "        encReplace = enc.transform(datasetDate[:, [i]])\n",
    "        datasetDate = np.concatenate([datasetDate[:, :i], encReplace, datasetDate[:, i+1:]], axis=1)\n",
    "\n",
    "    datasetDate = datasetDate.astype(float)\n",
    "    #print(datasetDate.dtype)\n",
    "    return datasetDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e602ad4-7641-4cda-8852-4465dfa3e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def garmentEncodeCategorical(categoricalToEncode):\n",
    "    '''Encode categorical data for the garment dataset'''\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    import numpy as np\n",
    "    #do ordinal encoding for categorical data\n",
    "    for i in []:\n",
    "        oec = OrdinalEncoder(categories='auto', dtype=float)\n",
    "        oec.fit(categoricalToEncode[:, [i]])\n",
    "\n",
    "        # replace original data in the dataset with encoded values\n",
    "        categoricalToEncode[:, i] = oec.transform(categoricalToEncode[:, [i]]).flatten()\n",
    "    \n",
    "    #do one hot encoding for categorical data\n",
    "    for i in [2,1,0]:\n",
    "        enc = OneHotEncoder(categories='auto', dtype=float, sparse=False)\n",
    "        enc.fit(categoricalToEncode[:, [i]])\n",
    "        encReplace = enc.transform(categoricalToEncode[:, [i]])\n",
    "        categoricalToEncode = np.concatenate([categoricalToEncode[:, :i], encReplace, categoricalToEncode[:, i+1:]], axis=1)\n",
    "        \n",
    "    categoricalToEncode = categoricalToEncode.astype(float)\n",
    "    #print(categoricalToEncode)\n",
    "    return categoricalToEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd8bf40-478e-47c1-b465-b8824f937b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateAndImputeNumericalGarment(datasetDate, datasetCategorical, datasetNumerical):\n",
    "    '''concatenate seperated garment columns together, and perform iterative imputation to fill in missing numeric values'''\n",
    "    import numpy as np\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    \n",
    "    dataset = np.concatenate([datasetDate, datasetCategorical, datasetNumerical], axis=1)\n",
    "    \n",
    "    #input missing numerical data using iterative imputing\n",
    "    imputer = IterativeImputer(random_state=17)\n",
    "    dataset = imputer.fit_transform(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "289eab7e-0206-436a-b80f-def7d5eb3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final preprocessing for garment dataset\n",
    "def pruneGarment(dataset, target):\n",
    "    '''remove any constant features from the garment dataset, and select the 13 best features to be used for machine learning'''\n",
    "    import numpy as np\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import r_regression\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    \n",
    "    #remove constant features\n",
    "    selector = VarianceThreshold()\n",
    "    dataset = selector.fit_transform(dataset)\n",
    "    #print(selector.variances_)\n",
    "    #print(dataset.shape)\n",
    "    \n",
    "    gwp_pr = r_regression(dataset,target)\n",
    "    #print(gwp_pr)\n",
    "        \n",
    "    #select best 13 features using pearson coefficient, as testing has shown that to be optimal amount of features\n",
    "    skb = SelectKBest(r_regression, k=13)\n",
    "    datasetSelected = skb.fit_transform(dataset, target) # dataset with selected features\n",
    "    return datasetSelected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e62aa0-5008-4e83-9fff-18490bf42c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test number of features for garment\n",
    "def testGarment():\n",
    "    '''testing function used to decide how many features should be kept for the garment dataset using pearson correlation coefficient'''\n",
    "    accuracyPlot =[]\n",
    "    prev_train_acc = 0\n",
    "    prev_test_acc = 0\n",
    "    for f in range(1, garmentDataset.shape[1]+1):\n",
    "        # feature selection\n",
    "        skb = SelectKBest(r_regression, k=f)\n",
    "        selectedDataset = skb.fit_transform(garmentDataset, garmentTarget) # dataset with selected features\n",
    "\n",
    "        # split the dataset into training and testing\n",
    "        x_train, x_test, y_train, y_test = train_test_split(selectedDataset, garmentTarget, test_size=0.2)\n",
    "\n",
    "        # normalize the training data\n",
    "        mms = MinMaxScaler(feature_range=(-1, 1))\n",
    "        x_train_norm = mms.fit_transform(x_train)\n",
    "\n",
    "        # train a classifier\n",
    "        model = LinearRegression()\n",
    "        model.fit(x_train_norm, y_train) # train a model using the training data.\n",
    "        train_acc = model.score(x_train_norm, y_train) # estimate training accuracy\n",
    "\n",
    "        # testing\n",
    "        x_test_norm = mms.transform(x_test) # normalize the testing data before testing. Note that we use the model fit using the training data\n",
    "        test_acc = model.score(x_test_norm, y_test) # estimate testing accuracy\n",
    "        accuracyPlot.append(test_acc)\n",
    "\n",
    "        print(f'Using {f} features:')\n",
    "        print('Training Accuracy: ', train_acc) # Accuracy on the training data\n",
    "        print('Testing Accuracy: ', test_acc) # Accuracy on the testing data\n",
    "\n",
    "\n",
    "    #plot a chart\n",
    "    featuresPlot = range(1, garmentDataset.shape[1]+1)\n",
    "    plt.plot(featuresPlot, accuracyPlot)\n",
    "    plt.title(\"Accuracy against features for garment\")\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2034051d-ad43-4940-bad0-266bc6f58aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStarsInitial():\n",
    "    '''get the initial stars dataset from the csv file'''\n",
    "    import numpy as np\n",
    "    #get dataset and create arrays of features and labels\n",
    "    #convert all features to float in order to create 2d array\n",
    "    #get array of features\n",
    "    initFeatures = np.genfromtxt(\"star_assessment.csv\", delimiter=\",\", dtype=\"f8\", encoding=None, missing_values='', filling_values=np.nan, usecols=range(0,17), skip_header=1)\n",
    "    #get array of labels\n",
    "    target = np.genfromtxt(\"star_assessment.csv\", delimiter=\",\", dtype=\"U25\", encoding=None, missing_values='', filling_values=None, usecols=17, skip_header=1)\n",
    "    #get class names\n",
    "    classes = np.genfromtxt(\"star_assessment.csv\", delimiter=\",\", dtype=\"U25\", encoding=None, missing_values='', filling_values=None, max_rows=1)\n",
    "    return initFeatures, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdfdb200-9daf-4521-b515-81f49dfc4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeStars(features):\n",
    "    '''use iterative imputation to fill in missing numeric values for the stars dataset'''\n",
    "    import numpy as np\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    #input missing data using iterative imputing\n",
    "    imputer = IterativeImputer(random_state=17)\n",
    "    features = imputer.fit_transform(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a27e27-1562-4e7e-95e3-7a72d5218664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeStars(labels):\n",
    "    '''encode the labels column for the stars dataset, so that we may use it for machine learning'''\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    #encode labels\n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(labels)\n",
    "    #print(list(enc.classes_))\n",
    "    labels = enc.transform(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0ee227d-0d72-429d-bb13-81ddb36e82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final preprocessing for stars dataset\n",
    "def pruneStars(dataset, target):\n",
    "    '''remove constant features from the stars dataset, and select the best 15 features to be used for machine learning'''\n",
    "    import numpy as np\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import r_regression\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    \n",
    "    #remove constant features\n",
    "    selector = VarianceThreshold()\n",
    "    dataset = selector.fit_transform(dataset)\n",
    "    #print(selector.variances_)\n",
    "    #print(dataset.shape)\n",
    "    \n",
    "    gwp_pr = r_regression(dataset,target)\n",
    "    #print(gwp_pr)\n",
    "        \n",
    "    #select best 15 features using pearson coefficient, as testing has shown that to be optimal amount of features\n",
    "    skb = SelectKBest(r_regression, k=15)\n",
    "    datasetSelected = skb.fit_transform(dataset, target) # dataset with selected features\n",
    "    return datasetSelected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b738fadf-5b09-471c-9756-755bec48b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testStars():\n",
    "    '''testing function used to decide how many features should be kept using pearson correlation coefficient'''\n",
    "    accuracyPlot =[]\n",
    "    prev_train_acc = 0\n",
    "    prev_test_acc = 0\n",
    "    for f in range(1, starsFeatures.shape[1]+1):\n",
    "        # feature selection\n",
    "        skb = SelectKBest(r_regression, k=f)\n",
    "        selectedFeatures = skb.fit_transform(starsFeatures, starsTarget) # dataset with selected features\n",
    "\n",
    "        # split the dataset into training and testing\n",
    "        x_train, x_test, y_train, y_test = train_test_split(selectedFeatures, starsTarget, test_size=0.2)\n",
    "\n",
    "        # normalize the training data\n",
    "        mms = MinMaxScaler(feature_range=(0, 1))\n",
    "        x_train_norm = mms.fit_transform(x_train)\n",
    "\n",
    "        # train a classifier\n",
    "        model = LinearSVC()\n",
    "        model.fit(x_train_norm, y_train) # train a model using the training data.\n",
    "        train_acc = model.score(x_train_norm, y_train) # estimate training accuracy\n",
    "\n",
    "        # testing\n",
    "        x_test_norm = mms.transform(x_test) # normalize the testing data before testing. Note that we use the model fit using the training data\n",
    "        test_acc = model.score(x_test_norm, y_test) # estimate testing accuracy\n",
    "        accuracyPlot.append(test_acc)\n",
    "\n",
    "        print(f'Using {f} features:')\n",
    "        print('Training Accuracy: ', train_acc) # Accuracy on the training data\n",
    "        print('Testing Accuracy: ', test_acc) # Accuracy on the testing data\n",
    "\n",
    "    #plot a chart\n",
    "    featuresPlot = range(1, starsFeatures.shape[1]+1)\n",
    "    plt.plot(featuresPlot, accuracyPlot)\n",
    "    plt.title(\"Accuracy against features for stars\")\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1f26e-06da-4081-88fc-1320a7ce7452",
   "metadata": {},
   "source": [
    "Markdown question for 3_2:\n",
    "The pearson correlation coefficient is used to test for linear correlation between two values. The closer the pearson correlation coefficient of a feature and the label is to 1, the greater the relevance of that feature in predicting the label. Therefore pearson correlation coefficient can also be used to find and weed out features which are shown to be unrelated to the label, as they will have a value close to 0. It is worth noting that a large negative value is in fact a good indicator of negative correlation, which is also useful for predicting the label.\n",
    "\n",
    "Therefore, the pearson correlation coefficient can be used to select only the features which are relevant to the label, in order to reduce noise in the dataset. This reduction in noise is likely to bring better results when the dataset is used to train a machine learning algorithm, as well as reducing the processing time needed to fit all features to the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
